---
title: "PominiÄ™cie istotnej zmiennej (nieskorelowanej z innymi)."
author: "Szymon Popkiewicz, Kacper Prorok"
date: "2024-03-30"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
set.seed(123)
library(tidyverse)
library(ggplot2)
library(nortest)
library(tseries)
library(knitr)

a<-2
b<-3
c<-5
n<-200
df<-data.frame("est_x1"=0,"stat_t_x1"=0,"ist_x1"=0,"r_adj"=0,"r"=0,"po_est_x1"=0,"po_stat_t_x1"=0,"po_ist_x1"=0,"po_r_adj"=0,"po_r"=0)
for(i in 1:1024)
{
  
  
  x1<-runif(n,0,20)
  x2<-runif(n,0,10)
  e<-rnorm(n,0,1)
  Y<-a*x1 + b*x2 +e + c
  #przed
  model=lm(Y~x1+x2)
  x1_est=as.numeric(summary(model)$coefficients[,1][2])
  t_x1<-(x1_est - a)/as.numeric(summary(model)$coefficients[,2][2])#statystyka t-studenta dla alfa=0.05
  ist_x1<-abs(t_x1)>qt(1 - 0.05/2, n-3)
  r_adj<-summary(model)$adj.r.squared
  r<-summary(model)$r.squared
  
  #po
  model2=lm(Y~x1)
  po_x1_est=as.numeric(summary(model2)$coefficients[,1][2])
 po_t_x1<-(po_x1_est - a)/as.numeric(summary(model2)$coefficients[,2][2])#statystyka t-studenta dla alfa=0.05
  po_ist_x1<-abs(po_t_x1)>qt(1 - 0.05/2, n-2)
  po_r_adj<-summary(model2)$adj.r.squared
  po_r<-summary(model2)$r.squared
  df<-rbind(df,c(x1_est,t_x1,ist_x1,r_adj,r,po_x1_est,po_t_x1,po_ist_x1,po_r_adj,po_r))
  
  
}
df<-df[-1,]
rownames(df)<-NULL
```

# Opis projektu 

*Projekt 3, wersja 2*

Wygenerowaniu danych speÅ‚niajÄ…cych zaleÅ¼noÅ›Ä‡: ğ‘¦ = ğ‘0 + ğ‘1ğ‘¥1 + ğ‘2ğ‘¥2 + ğœ€ (gdzie ğ‘0=5,ğ‘1=2,ğ‘2=3), a nastÄ™pnie estymowaniu parametrÃ³w modelu:

ğ‘¦ = ğ›¼0 + ğ›¼1ğ‘¥1 + ğ›¼2ğ‘¥2 + ğœ€ i ğ‘¦ = ğ›½0 + ğ›½1ğ‘¥1 + ğœ€

**Jeden model zawiera dwie zmienne objaÅ›niajÄ…ce, a w drugim pomijamy jednÄ… istotnÄ… zmiennÄ….**


Dane sÄ… losowane z:

-dla x1 z rozkÅ‚adu jednostajnego (0,20)

-dla x2 z rozkÅ‚adu jednostajnego (0,20)

-dla e z rozkÅ‚adu N(0,1)





Na tej podstawie badamy wpÅ‚yw pominiÄ™cia istotnej zmiennej, a w szczegÃ³lnoÅ›ci zwracamy uwagÄ™ na:
 
â€¢ wpÅ‚yw na oszacowania parametrÃ³w modelu (porÃ³wnaÄ‡ oszacowania ğ›¼1i ğ›½1, zbadaÄ‡
rozkÅ‚ad i nieobciÄ…Å¼onoÅ›Ä‡ estymatorÃ³w),

â€¢ na bÅ‚Ä™dy estymatorÃ³w parametrÃ³w ğ›¼1i ğ›½1,

â€¢ na statystykÄ™ t w testach ğ»0:ğ›¼1 = ğ‘1 i ğ»0:ğ›½1 = ğ‘1 (jej rozkÅ‚ad, odsetek odrzuceÅ„ ğ»0)

â€¢ na wsp. determinacji (skorygowany wsp. determinacji) w obu modelach.



Dla uÅ‚atwienia czytania projektu warto zapamiÄ™taÄ‡, Å¼e:

*Estymator ğ›¼1 pochodzi z modelu z dwoma istotnymi zmiennymi objaÅ›niajÄ…cymi natomiast estymator ğ›½1 pochodzi z modelu z pominiÄ™tÄ… istotnÄ… zmiennÄ….*




# WpÅ‚yw na oszacowania parametrÃ³w modelu


NastÄ™pujÄ…cy wykres przedstawia rozkÅ‚ad naszych dwÃ³ch estymatorÃ³w ğ›¼1,ğ›½1, gdy wspÃ³Å‚czynniki przy naszym modelu wynosiÅ‚y ğ‘0=5,ğ‘1=2,ğ‘2=3.


```{r}
df%>%
  pivot_longer(cols=c("est_x1","po_est_x1"),names_to = "przed_po",values_to = "oszacowania")%>%
  ggplot(aes(oszacowania,fill=przed_po))+
  geom_density(alpha=0.5)+
  labs(title="RozkÅ‚ad estymatorÃ³w parametrÃ³w",
       fill="Estymatory")+
  scale_fill_manual(labels = c("ğ›¼1","ğ›½1"),values = c("pink","blue"))

```

Badanie normalnoÅ›ci powyÅ¼szych rozkÅ‚adÃ³w:

```{r}
cat("ğ›¼1: Shapiro-Wilk", shapiro.test(df$est_x1)$p.value,
    "\nğ›¼1: Lillieforse", lillie.test(df$po_est_x1)$p.value,
    "\nğ›½1: Shapiro-Wilk", shapiro.test(df$est_x1)$p.value,
    "\nğ›½1: Lillieforse", lillie.test(df$po_est_x1)$p.value)

```

Wnioski:

- Testy normalnoÅ›ci we wszystkich przypadkach wykazaÅ‚y, Å¼e rozkÅ‚ad jest zgodny z rozkÅ‚adem normalnym. UsuniÄ™cie istotnej zmiennej objaÅ›niajÄ…cej nie wpÅ‚yneÅ‚o na wyniki testÃ³w.

- Po usuniÄ™ciu istotnej zmiennej losowej widzimy, Å¼e rozkÅ‚ad estymatora jest duÅ¼o bardziej pÅ‚aski i jest spore prawdopodobieÅ„stwo Å¼e bÄ™dzie doÅ›Ä‡ istotnie rÃ³Å¼ny od 2. W przypadku gdy w modelu mamy dwie istotne zmienne rozkÅ‚ad estymatora jest duÅ¼o bardziej "zbity" i przewaÅ¼nie bÄ™dzie bardzo blisko prawdziwej wartoÅ›ci.


## Badanie obciÄ…Å¼onoÅ›ci



```{r}
cat("wartoÅ›Ä‡ oczekiwana ğ›¼1: ",mean(df$est_x1),"\nwartoÅ›Ä‡ oczekiwana ğ›½1: ",mean(df$po_est_x1), "\nrelative_error_ğ›¼1:",mean(abs((a - df$est_x1) /a)), "\nrelative_error_ğ›½1:",mean(abs((a - df$po_est_x1) /a)),"\nMSE_ğ›¼1:",mean((a - df$est_x1)^2),"\nMSE_ğ›½1:",mean((a - df$po_est_x1)^2))

```


Wnioski:

- WartoÅ›Ä‡ oczekiwana w obydwÃ³ch przypadkach jest bardzo podobna

- Relative error parametru ğ›½1 jest wiÄ™kszy od ğ›¼1, wiÄ™c estymator bÄ™dzie popeÅ‚niaÅ‚ wiÄ™kszy bÅ‚Ä…d oszacowania

- BÅ‚Ä…d Å›redniokwartylowy (MSE) rÃ³wnieÅ¼ jest wiÄ™kszy w przypadku estymatora ğ›½1, czyli jest on mniej dokÅ‚adny niÅ¼ estymator ğ›¼1


# Statystyka t w testach 

```{r}

df%>%
  pivot_longer(cols=c("stat_t_x1","po_stat_t_x1"),names_to = "statystyki_t",values_to = "wartosci")%>%
  ggplot(aes(wartosci,color=statystyki_t))+
  geom_density(alpha=0.5,linewidth=1)+
  labs(title="RozkÅ‚ady statystyk t w tescie 'estymator = b1' ",fill="Estymatory",x="WartoÅ›ci statystyki")+
  scale_color_manual(labels = c("ğ›½1","ğ›¼1"),values = c("blue","pink"))
```

Badanie normalnoÅ›ci powyÅ¼szych rozkÅ‚adÃ³w:

```{r}
cat("PRZED: Shapiro-Wilk", shapiro.test(df$stat_t_x1)$p.value,
    "\nPRZED: Lillieforse", lillie.test(df$po_stat_t_x1)$p.value,
    "\nPO: Shapiro-Wilk", shapiro.test(df$stat_t_x1)$p.value,
    "\nPO: Lillieforse", lillie.test(df$po_stat_t_x1)$p.value)
```

Ponownie testy normalnoÅ›ci wykazaÅ‚y, Å¼e wszystkie rozkÅ‚ady sÄ… zgodne z rozkÅ‚adem normalnym.

## Hipotezy testÃ³w:



H0: ğ›¼1 = b1

H1: ğ›¼1 != b1


Dla ğ›¼1 (wersja z dwoma istotnymi zmiennymi):

```{r}
table(df$ist_x1)

```

H0: ğ›½1 = b1

H1: ğ›½1 != b1


Dla ğ›½1 (wersja z pominiÄ™tÄ… zmiennÄ… istotnÄ…):

```{R}
table(df$po_ist_x1)

```
1 - oszacowany parametr rÃ³Å¼ni siÄ™ istotnie od odpowiadajÄ…cej wartoÅ›ci teoretycznej (b1)

0 - oszacowany parametr nie rÃ³Å¼ni siÄ™ istotnie od odpowiadajÄ…cej wartoÅ›ci teoretycznej (b1)

Wnioski:

- RozkÅ‚ad wartoÅ›ci statystyki nie rÃ³Å¼ni siÄ™ zbytnio w obydwÃ³ch przypadkach i praktycznie wiÄ™kszoÅ›Ä‡ wartoÅ›ci znajduje siÄ™ w przedziale od -2 do 2, co oznacza Å¼e przyjmujemy hipotezÄ™, Å¼e estymator=b1 

- W tabelkach potwierdza siÄ™, to co widzimy na wykresie gÄ™stoÅ›ci - praktycznie w kaÅ¼dym przypadku zostaÅ‚a przyjÄ™ta hipoteza H0, niezaleÅ¼nie czy model miaÅ‚ dwie zmienne objaÅ›niajÄ…ce czy jednÄ…


# WpÅ‚yw na wspÃ³Å‚czynnik determinacji R 


```{r}
df%>%
  pivot_longer(cols=c("po_r","r"),names_to = "regular_R_squared",values_to = "wartosci")%>%
  ggplot(aes(wartosci,y=regular_R_squared,color=regular_R_squared))+
  geom_jitter(alpha=.5)+
  geom_boxplot(alpha=0.5)+
  labs(title="RozkÅ‚ady R.squared dla symulowanych modeli",
       x="WartoÅ›ci R squared",
       y="",
       color="estymator")+
  scale_color_manual(labels = c("ğ›½1","ğ›¼1"),values = c("blue","pink"))
```



## WpÅ‚yw na skorygowany wspÃ³Å‚czynnik determinacji R 

```{r}

df%>%
  pivot_longer(cols=c("po_r_adj","r_adj"),names_to = "adjusted_R_squared",values_to = "wartosci")%>%
  ggplot(aes(wartosci,y=adjusted_R_squared,color=adjusted_R_squared))+
  geom_jitter(alpha=.5)+
  geom_boxplot(alpha=0.5)+
  labs(title="RozkÅ‚ady adj.R.squared dla symulowanych modeli",
       x="WartoÅ›ci R squared adjusted",
       y="",
       color="estymator")+
  scale_color_manual(labels = c("ğ›½1","ğ›¼1"),values = c("blue","pink"))

```


Wnioski;

- WspÃ³Å‚czynnik R^2 w przypadku gdy mamy model z dwoma istotnymi zmiennymi prakytcznie zawsze wynosi 1, co oznacza, Å¼e model bardzo dobrze szacuje wartoÅ›ci. 

- W przypadku wariantu, gdy pomijamy jednÄ… istotnÄ… zmiennÄ… model nie jest juÅ¼ tak dokÅ‚adny i R^2 jest duÅ¼o niÅ¼sze. W naszym przypadku wiÄ™kszoÅ›Ä‡ wartoÅ›ci R^2 oscyluje wokÃ³Å‚ wartoÅ›ci 0.65.

- Skorygowany wspÃ³Å‚czynnik determinacji R zachowuje siÄ™ bardzo podobnie jak wspÃ³Å‚czynnik R.


# Zmiana wartoÅ›ci wspÃ³Å‚czynnika przy drugiej istotnej zmiennej 

```{r}

df<-data.frame("est_x1"=0,"stat_t_x1"=0,"ist_x1"=0,"r_adj"=0,"r"=0,"po_est_x1"=0,"po_stat_t_x1"=0,"po_ist_x1"=0,"po_r_adj"=0,"po_r"=0,"b"=0)

for(j in c(-60,-30,-6,-2,-0.5,0.5,2,6,30,60)){
  for(i in 1:1024)
  {
    
    x1<-runif(n,0,20)
    x2<-runif(n,0,10)
    e<-rnorm(n,0,1)
    Y<-a*x1 + j*x2 +e + c
    #przed
    model=lm(Y~x1+x2)
    x1_est=as.numeric(summary(model)$coefficients[,1][2])
    t_x1<-(x1_est - a)/as.numeric(summary(model)$coefficients[,2][2])#statystyka t-studenta dla alfa=0.05
    ist_x1<-abs(t_x1)>qt(1 - 0.05/2, 50-3)
    r_adj<-summary(model)$adj.r.squared
    r<-summary(model)$r.squared
    
    #po
    model2=lm(Y~x1)
    po_x1_est=as.numeric(summary(model2)$coefficients[,1][2])
   po_t_x1<-(po_x1_est - a)/as.numeric(summary(model2)$coefficients[,2][2])#statystyka t-studenta dla alfa=0.05
    po_ist_x1<-abs(po_t_x1)>qt(1 - 0.05/2, 50-3)
    po_r_adj<-summary(model2)$adj.r.squared
    po_r<-summary(model2)$r.squared
    df<-rbind(df,c(x1_est,t_x1,ist_x1,r_adj,r,po_x1_est,po_t_x1,po_ist_x1,po_r_adj,po_r,j))
    
    
}
}
df<-df[-1,]
rownames(df)<-NULL

```


Na koniec pokazujemy jak istotny wpÅ‚yw na nasze modele ma wielkoÅ›Ä‡ wspÃ³Å‚czynnika b1, ktÃ³ry stoi przy istotnej zmiennej objaÅ›niajÄ…cej, ktÃ³rÄ… w drugim modelu pomijamy. Wykresy poniÅ¼ej ukazujÄ… rozkÅ‚ady wspÃ³Å‚czynnika R dla modelu drugiego,ktÃ³re skategoryzowane sÄ… przez wielkoÅ›Ä‡ wspÃ³Å‚czynnika b1:

```{r}
ggplot(df,aes(x=po_r))+
  geom_density(fill="red")+
  facet_wrap(~b,nrow=3)+
  labs(title="RozkÅ‚ady R squared",
       x="R squared",
       fill="wartoÅ›Ä‡ b")
```


Na poniÅ¼szych wykresach pokazujemy jeszcze jak bardzo rÃ³Å¼ni siÄ™ rozkÅ‚ad estymatora ğ›½1 w zaleÅ¼noÅ›ci od wielkoÅ›ci wspÃ³Å‚czynnika b1:


```{r message=FALSE, warning=FALSE}

df%>%
  ggplot(aes(po_est_x1))+
  geom_density(alpha=0.5,fill="pink")+
  ylim(0,3)+
  xlim(-2.5,6.5)+
  facet_wrap(~b,ncol = 3)+
  labs(title="RozkÅ‚ad estymatorÃ³w parametrÃ³w dla ğ›½1, w zaleÅ¼noÅ›ci od wspÃ³Å‚czynnika przy X2")
```


Wnioski:

- Wraz ze wzrostem wartoÅ›ci bezwzglÄ™dnej maleje R squared dla modelu z pominiÄ™tÄ… istotnÄ… zmiennÄ… - jest to spowodowane tym, Å¼e im mocniejszy wpÅ‚yw ta druga istotna zmienna miaÅ‚a na model tym bardziej bÄ™dzie zaburzaÄ‡ to estymacje modelu w ktÃ³rym jÄ… pomijamy.

- Wraz ze wzrostem wartoÅ›Ä‡i bezwzglÄ™dnej wspÃ³Å‚czynnika roÅ›nie wariancja (rozkÅ‚ad robi siÄ™ bardziej pÅ‚aski), natomiast przy spadku wartoÅ›ci bezwzglÄ™dnej wspÃ³Å‚czynnika maleje wariancja (rozkÅ‚ad jest bardzej szpiczasty i skupiony wokÃ³Å‚ jednej wartoÅ›ci). 


**UtworzyliÅ›my teÅ¼ kilka wykresÃ³w, ktÃ³re moÅ¼na samemu ustawiaÄ‡, aby zrozumieÄ‡ co dzieje siÄ™ przy zmianach wspÃ³Å‚czynnikÃ³w. ZnajdujÄ… siÄ™ one pod tym linkiem:  https://2yo13d-kacpro94.shinyapps.io/Projekt3/**
